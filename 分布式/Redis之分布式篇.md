# 1 为什么需要Redis集群

## 1.1 性能

Redis 本身的 QPS 已经很高了，但是如果在一些并发量非常高的情况下，性能还是会受到影响。这个时候我们希望有更多的 Redis 服务来完成工作。 

## 1.2 扩展

第二个是出于存储的考虑。因为 Redis 所有的数据都放在内存中，如果数据量大，很容易受到硬件的限制。升级硬件收效和成本比太低，所以我们需要有一种横向扩展的方法。 

## 1.3 可用性

第三个是可用性和安全的问题。如果只有一个 Redis 服务，一旦服务宕机，那么所有的客户端都无法访问，会对业务造成很大的影响。另一个，如果硬件发生故障，而单机的数据无法恢复的话，带来的影响也是灾难性的。 

可用性、数据安全、性能都可以通过搭建多个 Reids 服务实现。其中有一个是主节点（master），可以有多个从节点(slave)。主从之间通过数据同步，存储完全相同的数据。如果主节点发生故障，则把某个从节点改成主节点，访问新的主节点。

# 2 Redis 主从复制(replication)

<img src="Redis之分布式篇.assets/20200612125328405.png" alt="img" style="zoom:50%;" />

## 2.1 主从复制配置

修改192.168.11.140和192.168.11.141的redis.conf文件：

slaveof masterip masterport redis5.0只有弃用了 slave 语法，改用replica-announce-ip 和 replica-announce-port

```markdown
	slaveof 192.168.11.138 6379

# 在主从切换的时候，这个配置会被重写成
# Generated by CONFIG REWRITE（slave代表奴隶）
	replicaof 192.168.11.138 6379
# 或者在启动服务时通过参数指定 master 节点
	./redis-server --slaveof 192.168.8.203 6379 
# 或在客户端直接执行 slaveof xx xx，使该 Redis 实例成为从节点
```

启动后，可以通过`info replication`查看：

```java
127.0.0.1:6379> info replication
# Replication
role:master
connected_slaves:2
master_replid:e1e1d9ba4ffe4075dbfc5bf92d87a4aa0290b220
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:0
second_repl_offset:-1
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
```

从节点不能写入数据（只读），只能从 master 节点同步数据。get 成功，set 失败

```shell
127.0.0.1:6379> set gupao 666 
(error) READONLY You can't write against a read only replica.
```

主节点写入后，slave 会自动从 master 同步数据。 

断开复制： `slaveof no one`

此时从节点会变成自己的主节点，不再复制数据。

## 2.2 主从复制原理

### 2.2.1 连接阶段

1. slave node 启动时（执行 slaveof 命令），会在自己本地保存 master node 的 信息，包括 master node 的 host 和 ip。 

2. slave node 内部有个定时任务 replicationCron（源码 replication.c），每隔 1 秒钟检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接(会向master发送一个SYNC的命令)，如果连接成功，从节点为该 socket 建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收 RDB 文件、接收命令传播等。 

当从节点变成了主节点的一个客户端之后，会给主节点发送 ping 请求。

![img](Redis之分布式篇.assets/2020061213453976.png)

### 2.2.2 数据同步阶段

3. master node 第一次执行全量复制，通过 bgsave 命令在本地生成一份 RDB 快照，将 RDB 快照文件发给 slave node（如果超时会重连，可以调大 repl-timeout 的值）。slave node 首先清除自己的旧数据，然后用 RDB 文件加载数据。 

**问题：生成 RDB 期间，master 接收到的命令怎么处理？**

开始生成 RDB 文件时，master 会把所有新的写命令缓存在内存中。在 slave node 保存了 RDB 之后，再将新的写命令复制给 slave node。 

### 2.2.3 命令传播阶段

4. master node 持续将写命令，异步复制给 slave node 

延迟是不可避免的，只能通过优化网络。 

`repl-disable-tcp-nodelay no`

当设置为 yes 时，TCP 会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与 Linux 内核的配置有关，默认配置为 40ms。当设置为 no 时，TCP 会立马将主节点的数据发送给从节点，带宽增加但延迟变小。

一般来说，只有当应用对 Redis 数据不一致的容忍度较高，且主从节点之间网络状 况不好时，才会设置为 yes；多数情况使用默认值 no。 

**问题：如果从节点有一段时间断开了与主节点的连接是不是要重新全量复制一遍？如果可以增量复制，怎么知道上次复制到哪里？**

通过 master_repl_offset 记录的偏移量 

```shell
redis> info replication
master_repl_offset:2345632
```

## 2.3 主从复制的不足

主从模式解决了数据备份和性能（通过读写分离）的问题，但是还是存在一些不足： 

1. RDB 文件过大的情况下，同步非常耗时

2. 在一主一从或者一主多从的情况下，如果主服务器挂了，对外提供的服务就不可用了，单点问题没有得到解决。如果每次都是手动把之前的从服务器切换成主服务器，这个比较费时费力，还会造成一定时间的服务不可用。 

# 3 可用性保证之 Sentinel

## 3.1 Sentinel 原理

如何实现主从的自动切换？我们的思路： 

创建一台监控服务器来监控所有 Redis 服务节点的状态，比如，master 节点超过一定时间没有给监控服务器发送心跳报文，就把 master 标记为下线，然后把某一个 slave 变成 master。应用每一次都是从这个监控服务器拿到 master 的地址。 

问题是：如果监控服务器本身出问题了怎么办？那我们就拿不到 master 的地址了，应用也没有办法访问。 

那我们再创建一个监控服务器，来监控监控服务器……似乎陷入死循环了，这个问题怎么解决？这个问题先放着。 

Redis 的 Sentinel 就是这种思路：通过运行监控服务器来保证服务的可用性。 

官网： https://redis.io/topics/sentinel 

从 Redis2.8 版本起，提供了一个稳定版本的 Sentinel（哨兵），用来解决高可用的问题。它是一个特殊状态的 redis 实例。 

我们会启动一个或者多个 Sentinel 的服务（通过 src/redis-sentinel），它本质上只是一个运行在特殊模式之下的 Redis，Sentinel 通过 info 命令得到被监听 Redis 机器的 master，slave 等信息。 

![image-20201108185843196](Redis之分布式篇.assets/image-20201108185843196.png)

为了保证监控服务器的可用性，我们会对 Sentinel 做集群的部署。Sentinel 既监控所有的 Redis 服务，<font color=red>Sentinel 之间也相互监控。</font> 

注意：Sentinel 本身没有主从之分，只有 Redis 服务节点有主从之分。 

概念梳理：master，slave（redis group），sentinel，sentinel 集合

### 3.1.1 服务下线

Sentinel 默认以每秒钟 1 次的频率向 Redis 服务节点发送 PING 命令。如果在 down-after-milliseconds 内都没有收到有效回复，Sentinel 会将该服务器标记为下线（`主观下线`）。 

```yml
# sentinel.conf 
sentinel down-after-milliseconds <master-name> <milliseconds> 
```

这个时候 Sentinel 节点会继续询问其他的 Sentinel 节点，确认这个节点是否下线，如果多数 Sentinel 节点都认为 master 下线，master 才真正确认被下线（`客观下线`），这个时候就需要重新选举 master。 

### 3.1.2 故障转移

如果 master 被标记为下线，就会开始故障转移流程。 

既然有这么多的 Sentinel 节点，由谁来做故障转移的事情呢？ 

故障转移流程的第一步就是在 Sentinel 集群选择一个 Leader，由 Leader 完成故障转移流程。Sentinle 通过 Raft 算法，实现 Sentinel 选举。 

#### Ratf 算法

在分布式存储系统中，通常通过维护多个副本来提高系统的可用性，那么多个节点 之间必须要面对数据一致性的问题。Raft 的目的就是通过复制的方式，使所有节点达成一致，但是这么多节点，以哪个节点的数据为准呢？所以必须选出一个 Leader。 

大体上有两个步骤：领导选举，数据复制。 

Raft 是一个共识算法（consensus algorithm）。比如比特币之类的加密货币，就需要共识算法。Spring Cloud 的注册中心解决方案 Consul 也用到了 Raft 协议。 

Raft 的核心思想：先到先得，少数服从多数。 

Raft 算法演示：http://thesecretlivesofdata.com/raft/ 

总结： 

Sentinle 的 Raft 算法和 Raft 论文略有不同。 

1. master 客观下线触发选举，而不是过了 election timeout 时间开始选举。 

2. Leader 并不会把自己成为 Leader 的消息发给其他 Sentinel。其他 Sentinel 等待 Leader 从 slave 选出 master 后，检测到新的 master 正常工作后，就会去掉客观下线的标识，从而不需要进入故障转移流程。

#### 故障转移

**问题：怎么让一个原来的 slave 节点成为主节点？**

1. 选出 Sentinel Leader 之后，由 Sentinel Leader 向某个节点发送 slaveof no one 命令，让它成为独立节点。 

2. 然后向其他节点发送 slaveof x.x.x.x xxxx（本机服务），让它们成为这个节点的子节点，故障转移完成。 

**问题：这么多从节点，选谁成为主节点？**

关于从节点选举，一共有四个因素影响选举的结果，分别是断开连接时长、优先级排序、复制数量、进程 id。 

如果与哨兵连接断开的比较久，超过了某个阈值，就直接失去了选举权。如果拥有选举权，那就看谁的优先级高，这个在配置文件里可以设置（replica-priority 100）， 

数值越小优先级越高。 

如果优先级相同，就看谁从 master 中复制的数据最多（复制偏移量最大），选最多的那个，如果复制数量也相同，就选择进程 id 最小的那个。 

## 3.2 Sentinel 的功能总结

> Monitoring. Sentinel constantly checks if your master and slave instances are working as expected. 
>
> Notification. Sentinel can notify the system administrator, another computer programs, via an API, that something is wrong with one of the monitored Redis instances. 
>
> Automatic failover. If a master is not working as expected, Sentinel can start a failover process where a slave is promoted to master, the other additional slaves are reconfigured to use the new master, and the applications using the Redis server informed about the new address to use when connecting. 
>
> Configuration provider. Sentinel acts as a source of authority for clients service discovery: clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address.

`监控` Sentinel 会不断检查主服务器和从服务器是否正常运行。 

`通知` 如果某一个被监控的实例出现问题，Sentinel 可以通过 API 发出通知。 

`自动故障转移（failover）` 如果主服务器发生故障，Sentinel 可以启动故障转移过程。把某台服务器升级为主服务器，并发出通知。 

`配置管理` 客户端连接到 Sentinel，获取当前的 Redis 主服务器的地址。

## 3.3 Sentinel 实战

### 3.3.1 Sentinel 配置

为了保证 Sentinel 的高可用，Sentinel 也需要做集群部署，集群中至少需要三个 Sentinel 实例（推荐奇数个，防止脑裂）。 

| hostname | IP 地址   | 节点角色&端口                   |
| -------- | --------- | ------------------------------- |
| master   | 127.0.0.1 | Master：6379 / Sentinel : 26379 |
| slave1   | 127.0.0.1 | Slave ：6380 / Sentinel : 26380 |
| slave2   | 127.0.0.1 | Slave ：6381 / Sentinel : 26381 |

以 Redis 安装路径/usr/local/redis-5.0.7/为例。 

```markdown
# 新建etc目录
	cd /usr/local/redis-5.0.7/
	mkdir etc
	cd etc/
	mkdir redis_master # 作为master节点配置文件的目录
	mkdir redis_slave1 # 作为第一个slave节点配置文件的目录
	mkdir redis_slave2 # 作为第二个slave节点配置文件的目录
# 回到安装目录将配置文件拷贝对应目录下
	cp redis.conf /usr/local/redis-5.0.7/etc/redis_master
	cp redis.conf /usr/local/redis-5.0.7/etc/redis_slave1
	cp redis.conf /usr/local/redis-5.0.7/etc/redis_slave2
# 主节点配置
	bind：0.0.0.0 #Redis 默认只允许本机访问，把 bind 修改为 0.0.0.0 表示允许所有远程访问
	port：6379   
	protected-mode：no #关闭保护模式，可以外部访问
	daemonize：yes     #设置为后台启动
	pidfile "/var/run/redis_6379.pid" #redi pid存储位置，建议加上端口，便于区分
	logfile "/Users/spring_zhang/logs/redis/redis_6379.log" #日志文件存储位置
	requirepass：1234 #设置 redis 连接密码
	masterauth：1234  #slave 服务连接 master 的密码
# 从节点配置
	bind：0.0.0.0
	port：6380
	protected-mode：no
	daemonize：yes
	pidfile "/var/run/redis_6380.pid"
	logfile "/Users/spring_zhang/logs/redis/redis_6380.log"
	requirepass：1234
	masterauth：1234
	replicaof 127.0.0.1 6379   ##主节点的IP 端口
# 分别启动三个节点
	cd /usr/local/redis-5.0.7/src
	./redis-server ../etc/redis_master/redis.conf
	./redis-server ../etc/redis_slave1/redis.conf
	./redis-server ../etc/redis_slave2/redis.conf
# 连接主从节点查看信息
```

<img src="Redis之分布式篇.assets/image-20201108222447941.png" alt="image-20201108222447941" style="zoom:67%;" />

<img src="Redis之分布式篇.assets/image-20201108222514732.png" alt="image-20201108222514732" style="zoom:67%;" />

```markdown
# 哨兵模式搭建'
# 同样为了方便管理，将sentinel.conf复制至对应的目录下
	cd /usr/local/redis-5.0.7
	cp sentinel.conf etc/redis_master/
	cp sentinel.conf etc/redis_slave1/
	cp sentinel.conf etc/redis_slave2/
# 分别编辑 sentinel.conf
	port:26379  # 默认端口为26379。
	protected-mode:no # 关闭保护模式，可以外部访问
	daemonize:yes     #设置为后台启动
	pidfile "/var/run/redis-sentinel_26379.pid" #redis sentinel pid文件位置
	logfile "/Users/spring_zhang/logs/redis/sentinel_26379.log" #日志文件
	sentinel monitor mymaster 127.0.0.1 6379 2  #指定主机IP地址和端口，并且指定当有2台哨兵认为主机挂了，则对主机进行容灾切换
	sentinel auth-pass mymaster 1234 #当在Redis实例中开启了requirepass，这里就需要提供密码
	sentinel down-after-milliseconds mymaster 3000 #这里设置了主机多少秒无响应，则认为挂了
	snetinel parallel-syncs mymaster 1 #主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的1
	sentinel failover-timeout mymaster 180000 #故障转移的超时时间，这里设置为三分钟
# 启动三个哨兵
	cd /usr/local/redis-5.0.7/src
	redis-sentinel ../etc/redis_master/sentinel.conf
	redis-sentinel ../etc/redis_slave1/sentinel.conf
	redis-sentinel ../etc/redis_slave2/sentinel.conf
```

上面出现了 4 个'mymaster'，这个名称要统一，并且使用客户端（比如 Jedis）连接的时候名称要正确。 

| 属性名                            | 作用                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| protected-mode                    | 是否允许外部网络访问                                         |
| dir                               | sentinel 的工作目录                                          |
| sentinel monitor                  | sentinel 监控的 redis 主节点                                 |
| down-after-milliseconds（毫秒）   | master 宕机多久，才会被 Sentinel 主观认为下线                |
| sentinel failover-timeout（毫秒） | 1 同一个 sentinel 对同一个 master 两次 failover 之间的间隔时间。 <br>2. 当一个 slave 从一个错误的 master 那里同步数据开始计算时间。直到slave 被纠正为向正确的 master 那里同步数据时。 <br>3.当想要取消一个正在进行的 failover 所需要的时间。<br>4.当进行 failover 时，配置所有 slaves 指向新的 master 所需的最大时间。 |
| parallel-syncs                    | 这个配置项指定了在发生 failover 主备切换时最多可以有多少个 slave 同时对新的 master 进行同步，这个数字越小，完成 failover 所需的时间就越长，但是如果这个数字越大，就意味着越 多的 slave 因为 replication 而不可用。可以通过将这个值设为 1 来保证每次只有一个 slave 处于不能处理命令请求的状态。 |

### 3.3.2 Sentinel 验证

模拟6379端口宕机。

6381端口被选为新的Master，只有一个 Slave 节点。

<img src="Redis之分布式篇.assets/image-20201108222547591.png" alt="image-20201108222547591" style="zoom:67%;" />

注意看 sentinel.conf 里面的 redis-master 被修改了！ 

![image-20201108232032297](Redis之分布式篇.assets/image-20201108232032297.png)

模拟原 master 恢复，在 6379 启动 redis-server。它还是 slave，但是 master 又有两个 slave 了。 

### 3.3.3 Sentinel 连接使用

Jedis 连接 Sentinel，master name 来自于 sentinel.conf 的配置。

```java
private static JedisSentinelPool createJedisPool() { 
  String masterName = "redis-master"; 
  Set<String> sentinels = new HashSet<String>(); 
  sentinels.add("192.168.8.203:26379"); 
  sentinels.add("192.168.8.204:26379"); 
  sentinels.add("192.168.8.205:26379"); 
  pool = new JedisSentinelPool(masterName, sentinels, "1234"); 
  return pool; 
}
public static void main(String[] args) {
    JedisSentinelPool pool = createJedisPool();
    System.out.println(pool.getCurrentHostMaster());
    pool.getResource().set("xiaoxiao", "520-"+System.currentTimeMillis());
    System.out.println(pool.getResource().get("xiaoxiao"));
}
```

Spring Boot 连接 Sentinel（springboot-redis：RedisAppTest.java） 

```yml
spring:
	redis:
  	sentinel:
    	master: mymaster
    	nodes: 127.0.0.1:26379,127.0.0.1:26380,127.0.0.1:26381
  password: 1234
```

无论是 Jedis 还是 Spring Boot（2.x 版本默认是 Lettuce），都只需要配置全部哨兵的地址，由哨兵返回当前的 master 节点地址。

**3.4 哨兵机制的不足** 

主从切换的过程中会丢失数据，因为只有一个 master。 

只能单点写，没有解决水平扩容的问题。 

如果数据量非常大，这个时候我们需要多个 master-slave 的 group，把数据分布到 不同的 group 中。 

问题来了，数据怎么分片？分片之后，怎么实现路由？ 

# 4 Redis 分布式方案

如果要实现 Redis 数据的分片，我们有三种方案。第一种是在客户端实现相关的逻辑，例如用取模或者一致性哈希对 key 进行分片，查询和修改都先判断 key 的路由。 

第二种是把做分片处理的逻辑抽取出来，运行一个独立的代理服务，客户端连接到 这个代理服务，代理服务做请求的转发。 

第三种就是基于服务端实现。

## 4.1 客户端 Sharding

![image-20201108191519032](Redis之分布式篇.assets/image-20201108191519032.png)

Jedis 客户端提供了 Redis Sharding 的方案，并且支持连接池。

```java
public class ShardingTest { 
  public static void main(String[] args) { 
    JedisPoolConfig poolConfig = new JedisPoolConfig(); 
    // Redis 服务器 
    JedisShardInfo shardInfo1 = new JedisShardInfo("127.0.0.1", 6379); 
    JedisShardInfo shardInfo2 = new JedisShardInfo("192.168.8.205", 6379); 
    // 连接池 
    List<JedisShardInfo> infoList = Arrays.asList(shardInfo1, shardInfo2); 
    ShardedJedisPool jedisPool = new ShardedJedisPool(poolConfig, infoList); 
    
    ShardedJedis jedis = null; 
    try{
      jedis = jedisPool.getResource(); 
      for(int i=0; i<100; i++){ 
        jedis.set("k"+i, ""+i); 
      }
      for(int i=0; i<100; i++){ 
        System.out.println(jedis.get("k"+i)); 
      } 
    }finally{ 
      if(jedis!=null) { 
        jedis.close(); 
      } 
    } 
  } 
}
```

使用 ShardedJedis 之类的客户端分片代码的优势是配置简单，不依赖于其他中间件，分区的逻辑可以自定义，比较灵活。但是基于客户端的方案，不能实现动态的服务增减，每个客户端需要自行维护分片策略，存在重复代码。 

第二种思路就是把分片的代码抽取出来，做成一个公共服务，所有的客户端都连接到这个代理层。由代理层来实现请求和转发。 

## 4.2 代理 Proxy

![image-20201108191751047](Redis之分布式篇.assets/image-20201108191751047.png)

典型的代理分区方案有 Twitter 开源的 Twemproxy 和国内的豌豆荚开源的 Codis。

## 4.3 Redis Cluster

Redis Cluster 是在 Redis 3.0 的版本正式推出的，用来解决分布式的需求，同时也可以实现高可用。跟 Codis 不一样，它是去中心化的，客户端可以连接到任意一个可用节点。

数据分片有几个关键的问题需要解决： 

1. 数据怎么相对均匀地分片 

2. 客户端怎么访问到相应的节点和数据 

3. 重新分片的过程，怎么保证正常服务

### 4.3.1 架构

Redis Cluster 可以看成是由多个 Redis 实例组成的数据集合。客户端不需要关注数据的子集到底存储在哪个节点，只需要关注这个集合整体。 

以 3 主 3 从为例，节点之间两两交互，共享数据分片、节点状态等信息。

![image-20201108192052014](Redis之分布式篇.assets/image-20201108192052014.png)

### 4.3.2 搭建 // TODO

### 4.3.3 数据分布

如果是希望数据分布相对均匀的话，我们首先可以考虑哈希后取模。 

#### 哈希后取模

例如，hash(key)%N，根据余数，决定映射到那一个节点。这种方式比较简单，属于静态的分片规则。但是一旦节点数量变化，新增或者减少，由于取模的 N 发生变化，数据需要重新分布。 

为了解决这个问题，我们又有了一致性哈希算法。 

#### 一致性哈希

一致性哈希的原理： 

把所有的哈希值空间组织成一个虚拟的圆环（哈希环），整个空间按顺时针方向组织。因为是环形空间，0 和 `2^32-1` 是重叠的。 

假设我们有四台机器要哈希环来实现映射（分布数据），我们先根据机器的名称或者 IP 计算哈希值，然后分布到哈希环中（红色圆圈）。

![image-20201108192255494](Redis之分布式篇.assets/image-20201108192255494.png)

现在有 4 条数据或者 4 个访问请求，对 key 计算后，得到哈希环中的位置（绿色圆圈）。沿哈希环顺时针找到的第一个 Node，就是数据存储的节点。 

![image-20201108192327938](Redis之分布式篇.assets/image-20201108192327938.png)

在这种情况下，新增了一个 Node5 节点，不影响数据的分布。

![image-20201108192351703](Redis之分布式篇.assets/image-20201108192351703.png)

删除了一个节点 Node4，只影响相邻的一个节点。

![image-20201108192419383](Redis之分布式篇.assets/image-20201108192419383.png)

谷歌的 MurmurHash 就是一致性哈希算法。在分布式系统中，负载均衡、分库分表等场景中都有应用。 

一致性哈希解决了动态增减节点时，所有数据都需要重新分布的问题，它只会影响到下一个相邻的节点，对其他节点没有影响。 

但是这样的一致性哈希算法有一个缺点，因为节点不一定是均匀地分布的，特别是在节点数比较少的情况下，所以数据不能得到均匀分布。解决这个问题的办法是引入`虚拟节点（Virtual Node）`。 

比如：2 个节点，5 条数据，只有 1 条分布到 Node2，4 条分布到 Node1，不均匀。

![image-20201108192525120](Redis之分布式篇.assets/image-20201108192525120.png)

Node1 设置了两个虚拟节点，Node2 也设置了两个虚拟节点（虚线圆圈）。 

这时候有 3 条数据分布到 Node1，1 条数据分布到 Node2。

![image-20201108192548548](Redis之分布式篇.assets/image-20201108192548548.png)

#### Redis 虚拟槽分区

Redis 既没有用哈希取模，也没有用一致性哈希，而是用虚拟槽来实现的。 

Redis 创建了 16384 个槽（slot），每个节点负责一定区间的 slot。比如 Node1 负责 0-5460，Node2 负责 5461-10922，Node3 负责 10923-16383。 

![image-20201108192659496](Redis之分布式篇.assets/image-20201108192659496.png)

Redis 的每个 master 节点维护一个 16384 位（2048bytes=2KB）的位序列，比如：序列的第 0 位是 1，就代表第一个 slot 是它负责；序列的第 1 位是 0，代表第二个 slot 不归它负责。 

对象分布到 Redis 节点上时，对 key 用 CRC16 算法计算再%16384，得到一个 slot 的值，数据落到负责这个 slot 的 Redis 节点上。 

查看 key 属于哪个 slot： 

```shell
redis> cluster keyslot qingshan 
```

<font color=red>注意：key 与 slot 的关系是永远不会变的，会变的只有 slot 和 Redis 节点的关系。 </font>

**问题：怎么让相关的数据落到同一个节点上？**

比如有些 multi key 操作是不能跨节点的，如果要让某些数据分布到一个节点上，例如用户 2673 的基本信息和金融信息，怎么办？

在 key 里面加入{hash tag}即可。Redis 在计算槽编号的时候只会获取{}之间的字符串进行槽编号计算，这样由于上面两个不同的键，{}里面的字符串是相同的，因此他们可 

以被计算出相同的槽。 

user{2673}base=… 

user{2673}fin=… 

```shell
127.0.0.1:7293> set a{qs}a 1 
OK 
127.0.0.1:7293> set a{qs}b 1 
OK 
127.0.0.1:7293> set a{qs}c 1 
OK
```

### 4.3.4 客户端重定向

比如在 7291 端口的 Redis 的 redis-cli 客户端操作： 

```shell
127.0.0.1:7291> set qs 1 
(error) MOVED 13724 127.0.0.1:7293 
```

服务端返回 MOVED，也就是根据 key 计算出来的 slot 不归 7191 端口管理，而是归 7293 端口管理，服务端返回 MOVED 告诉客户端去 7293 端口操作。 

这个时候更换端口，用 redis-cli –p 7293 操作，才会返回 OK。或者用`./redis-cli -c -p port`的命令（c 代表 cluster）。这样客户端需要连接两次。 

Jedis 等客户端会在本地维护一份 slot——node 的映射关系，大部分时候不需要重定向，所以叫做` smart jedis（需要客户端支持）`。 

### 4.3.5 数据迁移

因为 key 和 slot 的关系是永远不会变的，当新增了节点的时候，需要把原有的 slot 分配给新的节点负责，并且把相关的数据迁移过来。 

添加新节点（新增一个 7297）： 

```
redis-cli --cluster add-node 127.0.0.1:7291 127.0.0.1:7297 
```

新增的节点没有哈希槽，不能分布数据，在原来的任意一个节点上执行： 

```
redis-cli --cluster reshard 127.0.0.1:7291 
```

输入需要分配的哈希槽的数量（比如 500），和哈希槽的来源节点（可以输入 all 或 者 id）。

### 4.3.6 高可用和主从切换原理

当 slave 发现自己的 master 变为 FAIL 状态时，便尝试进行 Failover，以期成为新的master。由于挂掉的master可能会有多个slave，从而存在多个slave竞争成为master 

节点的过程， 其过程如下： 

1. slave 发现自己的 master 变为 FAIL 

2. 将自己记录的集群 currentEpoch 加 1，并广播 FAILOVER_AUTH_REQUEST 信息 

3. 其他节点收到该信息，只有 master 响应，判断请求者的合法性，并发送 FAILOVER_AUTH_ACK，对每一个 epoch 只发送一次 ack 

4. 尝试 failover 的 slave 收集 FAILOVER_AUTH_ACK 

5. 超过半数后变成新 Master 

6. 广播 Pong 通知其他集群节点

Redis Cluster 既能够实现主从的角色分配，又能够实现主从切换，相当于集成了 Replication 和 Sentinal 的功能。

### 4.3.7 总结

优势

1. 无中心架构。 

2. 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。 

3. 可扩展性，可线性扩展到 1000 个节点（官方推荐不超过 1000 个），节点可动态添加或删除。 

4. 高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave 到 Master 的角色提升。 

5. 降低运维成本，提高系统的扩展性和可用性。 

不足

1. Client 实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。 

2. 节点会因为某些原因发生阻塞（阻塞时间大于 clutser-node-timeout），被判断下线，这种 failover 是没有必要的。 

3. 数据通过异步复制，不保证数据的强一致性。 

4. 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。



## redis 缓存的更新


1. 先删除缓存， 再更新数据库 
2. 先更新数据库，更新成功后，让缓存失效 
3. 更新数据的时候， 只更新缓存，不更新数据库，然后同步异步调度去批量更新数据库

# 缓存击穿

## 缓存穿透

缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库中查询。 

解决思路：

1，如果查询数据库也为空，直接设置一个默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。

2，根据缓存数据Key的规则。例如我们公司是做机顶盒的，缓存数据以Mac为Key，Mac是有规则，如果不符合规则就过滤掉，这样可以过滤一部分查询。在做缓存规划的时候，Key有一定规则的话，可以采取这种办法。这种办法只能缓解一部分的压力，过滤和系统无关的查询，但是无法根治。

3，采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的BitSet中，不存在的数据将会被拦截掉，从而避免了对底层存储系统的查询压力。

大并发的缓存穿透会导致缓存雪崩。

## 缓存击穿

缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。

解决思路：

1，设置热点数据永远不过期

2，加互斥锁

## 缓存雪崩

缓存雪崩可能是因为数据未加载到缓存中，或缓存宕机，或者缓存同一时间大面积的失效，从而导致所有请求都去查数据库，导致数据库CPU和内存负载过高，甚至宕机。

解决思路：

1，采用加锁计数，或者使用合理的队列数量来避免缓存失效时对数据库造成太大的压力。这种办法虽然能缓解数据库的压力，但是同时又降低了系统的吞吐量。

2，分析用户行为，尽量让失效时间点均匀分布。避免缓存雪崩的出现。

3，如果是因为某台缓存服务器宕机，可以考虑做主备，比如：redis主备，但是双缓存涉及到更新事务的问题，update可能读到脏数据，需要好好解决。

## 缓存失效

如果缓存集中在一段时间内失效，DB的压力凸显。这个没有完美解决办法，但可以分析用户行为，尽量让失效时间点均匀分布。 

# redis 分布式锁

## 分布式锁的实现

锁是用来解决什么问题的：

1. 一个进程中的多个线程，多个线程并发访问同一个资源的时候，如何解决线程安全问题。 
2. 一个分布式架构系统中的两个模块同时去访问一个文件对文件进行读写操作 
3. 多个应用对同一条数据做修改的时候，如何保证数据的安全性

在进程中，我们可以用到synchronized、lock之类的同步操作去解决，但是对于分布式架构下多进程的情况下，如何做到跨进程的锁。就需要借助一些第三方手段来完成。

## 分布式锁的解决方案

### 数据库

通过唯一约束

```java
lock(
  id  int(11)
  methodName  varchar(100)
  memo varchar(1000) 
  modifyTime timestamp
  unique key mn (methodName)  --唯一约束
)
```

获取锁的伪代码

```java
try{
	exec insert into lock(methodName,memo) values(‘method’,’desc’);
	return true;
}Catch(DuplicateException e){
	return false;
}
```

释放锁

```java
delete from lock where methodName=''
```

#### 存在的需要思考的问题

1. 锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁 
2. 锁是非阻塞的，数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作 
3. 锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁

### zookeeper实现分布式锁

利用 zookeeper 的唯一节点特性或者有序临时节点特性获得最小节点作为锁. zookeeper 的实现相对简单，通过curator客户端，已经对锁的操作进行了封装，原理如下： 

![img](Redis之分布式篇.assets/20200611112108888.png)

#### zookeeper 的优势

1. 可靠性高、实现简单 
2. zookeeper因为临时节点的特性，如果因为其他客户端因为异常和zookeeper连接中断了，那么节点会被删除，意味着锁会被自动释放 
3. zookeeper本身提供了一套很好的集群方案，比较稳定 
4. 释放锁操作，会有watch通知机制，也就是服务器端会主动发送消息给客户端这个锁已经被释放了

### 基于缓存的分布式锁实现

redis中有一个setNx命令，这个命令只有在key不存在的情况下为key设置值。所以可以利用这个特性来实现分布式锁的操作。

1. 添加依赖

```XML
<dependency>
  <groupId>redis.clients</groupId>
  <artifactId>jedis</artifactId>
  <version>2.9.0</version>
</dependency>
```

1. 编写 redis 连接的代码

```java
public class RedisManager {

    private static JedisPool jedisPool;

    static {
        JedisPoolConfig jedisPoolConfig = new JedisPoolConfig();
        jedisPoolConfig.setMaxTotal(20);
        jedisPoolConfig.setMaxIdle(10);
        jedisPool = new JedisPool(jedisPoolConfig, "127.0.0.1", 6379, 10000, "1234");

    }

    public static Jedis getJedis() throws Exception {
        if (null != jedisPool) {
            return jedisPool.getResource();
        }
        throw new Exception("Jedispool was not init");
    }
}
```

1. 分布式锁的具体实现

```java
public class RedisLock {

    public String getLock(String key, int timeout) {
        try {
            Jedis jedis = RedisManager.getJedis();
            String value = UUID.randomUUID().toString();

            long end = System.currentTimeMillis() + timeout;
            // 阻塞
            while (System.currentTimeMillis() < end) {
                if (jedis.setnx(key, value) == 1) {
                    jedis.expire(key, timeout);
                    // 锁设置成功，redis 操作成功
                    return value;
                }
                if (jedis.ttl(key) == -1) {
                    jedis.expire(key, timeout);
                }
                Thread.sleep(1000);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
        return null;
    }

    public boolean releaseLock(String key, String value) {
        try {
            Jedis jedis = RedisManager.getJedis();
            while (true) {
                jedis.watch(key);
                // 判断获得锁的线程和当前 redis 中存的锁是同一个
                if (value.equals(jedis.get(key))) {
                    Transaction transaction = jedis.multi();
                    transaction.del(key);

                    List<Object> list = transaction.exec();
                    if (list == null) {
                        continue;
                    }
                    return true;
                }
                jedis.unwatch();
                break;
            }

        } catch (Exception e) {
            e.printStackTrace();
        }
        return false;
    }

    public static void main(String[] args) {
        RedisLock lock = new RedisLock();
        String lockId = lock.getLock("lock:first", 10000);
        if (null != lockId) {
            System.out.println("获得锁成功");
        }

        String lockId1 = lock.getLock("lock:first", 10000);
        System.out.println(lockId1);
    }
}
```

